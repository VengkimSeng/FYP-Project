{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba62b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "            TF-IDF FEATURE EXTRACTION FOR KHMER TEXT CLASSIFICATION             \n",
      "================================================================================\n",
      "Step 1: Loading processed documents...\n",
      "Loaded 15000 documents in 2.24 seconds\n",
      "\n",
      "Step 2: Creating pandas DataFrame...\n",
      "Category distribution:\n",
      "  - health: 2500 documents (16.7%)\n",
      "  - environment: 2500 documents (16.7%)\n",
      "  - technology: 2500 documents (16.7%)\n",
      "  - economic: 2500 documents (16.7%)\n",
      "  - sport: 2500 documents (16.7%)\n",
      "  - politic: 2500 documents (16.7%)\n",
      "\n",
      "Step 3 & 4: Creating and fitting TF-IDF vectorizer...\n",
      "TF-IDF vectorizer created with 21708 features in 1.07 seconds\n",
      "Vectorizer saved to: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/tfidf_vectorizer.pkl\n",
      "\n",
      "Step 5: Transforming texts to TF-IDF features...\n",
      "Created feature matrix with shape: (15000, 21708)\n",
      "Transformation completed in 1.02 seconds\n",
      "\n",
      "Step 6: Splitting data into training (70%) and validation (30%) sets...\n",
      "Training set: 10500 documents\n",
      "Validation set: 4500 documents\n",
      "\n",
      "Step 7: Encoding category labels...\n",
      "Label mapping:\n",
      "  0: economic\n",
      "  1: environment\n",
      "  2: health\n",
      "  3: politic\n",
      "  4: sport\n",
      "  5: technology\n",
      "Label encoder saved to: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/label_encoder.pkl\n",
      "\n",
      "Step 8: Transforming training and validation sets...\n",
      "Training features: (10500, 21708)\n",
      "Validation features: (4500, 21708)\n",
      "Features saved to output directory\n",
      "\n",
      "TF-IDF feature extraction and training data preparation complete!\n",
      "================================================================================\n",
      "\n",
      "Ready for model training with the following files:\n",
      "1. TF-IDF Vectorizer: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/tfidf_vectorizer.pkl\n",
      "2. Label Encoder: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/label_encoder.pkl\n",
      "3. Training Features: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/train_features.npz\n",
      "4. Validation Features: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/valid_features.npz\n",
      "5. Training Metadata: /Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features/tfidf_training_metadata.pkl\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "# Define paths\n",
    "PROCESSED_TEXTS_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/Preprocess_articles'\n",
    "METADATA_PATH = '/Users/socheata/Documents/FYP-Khmer-Classification/orginal_articles/metadata.csv'\n",
    "OUTPUT_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/TF_IDF_Features'\n",
    "MODEL_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/Models'\n",
    "# Create output directory if needed\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define tokenizer at module level (not inside a function)\n",
    "def tokenizer_split(text):\n",
    "    \"\"\"Simple tokenizer that splits on spaces (for preprocessed Khmer text)\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "# Step 1: Load training documents\n",
    "def load_processed_documents():\n",
    "    \"\"\"Load preprocessed Khmer documents with their categories\"\"\"\n",
    "    print(\"Step 1: Loading processed documents...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load metadata\n",
    "    metadata_df = pd.read_csv(METADATA_PATH)\n",
    "    doc_categories = dict(zip(metadata_df['docId'], metadata_df['category']))\n",
    "\n",
    "    # Initialize lists to store document data\n",
    "    doc_ids = []\n",
    "    doc_texts = []\n",
    "    categories = []\n",
    "\n",
    "    # Load all text files\n",
    "    text_files = [f for f in os.listdir(PROCESSED_TEXTS_DIR) if f.endswith('.txt')]\n",
    "\n",
    "    for filename in text_files:\n",
    "        # Get docId from filename\n",
    "        doc_id = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Skip if no category is available\n",
    "        if doc_id not in doc_categories:\n",
    "            continue\n",
    "\n",
    "        # Read text content\n",
    "        with open(os.path.join(PROCESSED_TEXTS_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Replace double newlines with space (combine title and body)\n",
    "        text = text.replace('\\n\\n', ' ')\n",
    "\n",
    "        # Store document information\n",
    "        doc_ids.append(doc_id)\n",
    "        doc_texts.append(text)\n",
    "        categories.append(doc_categories[doc_id])\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Loaded {len(doc_ids)} documents in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return doc_ids, doc_texts, categories\n",
    "\n",
    "# Step 2: Create pandas DataFrame\n",
    "def create_dataframe(doc_ids, doc_texts, categories):\n",
    "    \"\"\"Create a pandas DataFrame containing document data\"\"\"\n",
    "    print(\"\\nStep 2: Creating pandas DataFrame...\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id': doc_ids,\n",
    "        'text': doc_texts,\n",
    "        'cat': categories\n",
    "    })\n",
    "\n",
    "    # Display category distribution\n",
    "    print(\"Category distribution:\")\n",
    "    category_counts = df['cat'].value_counts()\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"  - {cat}: {count} documents ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 3 & 4: Create TF-IDF vectorizer, fit on data, and save\n",
    "def build_and_save_tfidf(texts):\n",
    "    \"\"\"Build TF-IDF vectorizer, fit on all texts, and save the vectorizer\"\"\"\n",
    "    print(\"\\nStep 3 & 4: Creating and fitting TF-IDF vectorizer...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create TF-IDF vectorizer (using the module-level tokenizer)\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenizer_split,  # Use simple space-based tokenization\n",
    "        token_pattern=None,        # Disable default token pattern to use custom tokenizer\n",
    "        encoding='utf-8',           # Ensure proper handling of UTF-8 (for Khmer)\n",
    "        ngram_range=(1, 1),         # Use unigrams and bigrams\n",
    "        min_df=2,                   # Minimum document frequency\n",
    "        max_df=0.85,                # Maximum document frequency\n",
    "        sublinear_tf=True           # Apply sublinear TF scaling\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on all texts\n",
    "    tfidf_vectorizer.fit(texts)\n",
    "\n",
    "    # Get vocabulary size\n",
    "    vocab_size = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "    # Save the fitted vectorizer\n",
    "    vectorizer_path = os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl')\n",
    "    with open(vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"TF-IDF vectorizer created with {vocab_size} features in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "# Step 5: Transform texts to TF-IDF features\n",
    "def transform_to_tfidf(tfidf_vectorizer, texts):\n",
    "    \"\"\"Transform texts to TF-IDF feature matrix\"\"\"\n",
    "    print(\"\\nStep 5: Transforming texts to TF-IDF features...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Transform texts to TF-IDF features\n",
    "    features = tfidf_vectorizer.transform(texts)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Created feature matrix with shape: {features.shape}\")\n",
    "    print(f\"Transformation completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# Step 6: Split data into training and validation sets\n",
    "def split_data(df, test_size=0.3):\n",
    "    \"\"\"Split data into training and validation sets\"\"\"\n",
    "    print(f\"\\nStep 6: Splitting data into training ({100-test_size*100:.0f}%) and validation ({test_size*100:.0f}%) sets...\")\n",
    "\n",
    "    # Split the dataset\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        df['text'],\n",
    "        df['cat'],\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=df['cat']  # Maintain class distribution in both sets\n",
    "    )\n",
    "\n",
    "    print(f\"Training set: {len(train_x)} documents\")\n",
    "    print(f\"Validation set: {len(valid_x)} documents\")\n",
    "\n",
    "    return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "# Step 7: Encode category labels\n",
    "def encode_labels(train_y, valid_y):\n",
    "    \"\"\"Encode category labels to integers\"\"\"\n",
    "    print(\"\\nStep 7: Encoding category labels...\")\n",
    "\n",
    "    # Create and fit label encoder\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    train_y_encoded = encoder.fit_transform(train_y)\n",
    "    valid_y_encoded = encoder.transform(valid_y)\n",
    "\n",
    "    # Save the encoder\n",
    "    encoder_path = os.path.join(OUTPUT_DIR, 'label_encoder.pkl')\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    # Display label mapping\n",
    "    label_mapping = {i: label for i, label in enumerate(encoder.classes_)}\n",
    "    print(\"Label mapping:\")\n",
    "    for index, label in label_mapping.items():\n",
    "        print(f\"  {index}: {label}\")\n",
    "\n",
    "    print(f\"Label encoder saved to: {encoder_path}\")\n",
    "\n",
    "    return train_y_encoded, valid_y_encoded, encoder\n",
    "\n",
    "# Step 8: Transform train and validation sets with TF-IDF\n",
    "def transform_train_valid_sets(tfidf_vectorizer, train_x, valid_x):\n",
    "    \"\"\"Transform training and validation sets with TF-IDF\"\"\"\n",
    "    print(\"\\nStep 8: Transforming training and validation sets...\")\n",
    "\n",
    "    # Transform training and validation sets\n",
    "    xtrain_tfidf = tfidf_vectorizer.transform(train_x)\n",
    "    xvalid_tfidf = tfidf_vectorizer.transform(valid_x)\n",
    "\n",
    "    print(f\"Training features: {xtrain_tfidf.shape}\")\n",
    "    print(f\"Validation features: {xvalid_tfidf.shape}\")\n",
    "\n",
    "    # Save the TF-IDF matrices\n",
    "    np.savez(os.path.join(OUTPUT_DIR, 'train_features.npz'),\n",
    "             data=xtrain_tfidf.data,\n",
    "             indices=xtrain_tfidf.indices,\n",
    "             indptr=xtrain_tfidf.indptr,\n",
    "             shape=xtrain_tfidf.shape)\n",
    "\n",
    "    np.savez(os.path.join(OUTPUT_DIR, 'valid_features.npz'),\n",
    "             data=xvalid_tfidf.data,\n",
    "             indices=xvalid_tfidf.indices,\n",
    "             indptr=xvalid_tfidf.indptr,\n",
    "             shape=xvalid_tfidf.shape)\n",
    "\n",
    "    print(\"Features saved to output directory\")\n",
    "\n",
    "    return xtrain_tfidf, xvalid_tfidf\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TF-IDF FEATURE EXTRACTION FOR KHMER TEXT CLASSIFICATION\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Steps 1: Load documents\n",
    "    doc_ids, doc_texts, categories = load_processed_documents()\n",
    "\n",
    "    # Step 2: Create DataFrame\n",
    "    df = create_dataframe(doc_ids, doc_texts, categories)\n",
    "\n",
    "    # Steps 3 & 4: Create, fit and save TF-IDF vectorizer\n",
    "    tfidf_vectorizer = build_and_save_tfidf(df['text'])\n",
    "\n",
    "    # Step 5: Transform to TF-IDF features\n",
    "    features = transform_to_tfidf(tfidf_vectorizer, df['text'])\n",
    "\n",
    "    # Step 6: Split data\n",
    "    train_x, valid_x, train_y, valid_y = split_data(df)\n",
    "\n",
    "    # Step 7: Encode labels\n",
    "    train_y_encoded, valid_y_encoded, encoder = encode_labels(train_y, valid_y)\n",
    "\n",
    "    # Step 8: Transform train and validation sets\n",
    "    xtrain_tfidf, xvalid_tfidf = transform_train_valid_sets(tfidf_vectorizer, train_x, valid_x)\n",
    "\n",
    "    # Save additional metadata for convenience\n",
    "    metadata = {\n",
    "        'num_documents': len(doc_ids),\n",
    "        'num_features': features.shape[1],\n",
    "        'num_classes': len(encoder.classes_),\n",
    "        'class_distribution': df['cat'].value_counts().to_dict(),\n",
    "        'train_size': len(train_x),\n",
    "        'valid_size': len(valid_x),\n",
    "        'classes': encoder.classes_.tolist()\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, 'tfidf_training_metadata.pkl'), 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(\"\\nTF-IDF feature extraction and training data preparation complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nReady for model training with the following files:\")\n",
    "    print(f\"1. TF-IDF Vectorizer: {os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl')}\")\n",
    "    print(f\"2. Label Encoder: {os.path.join(OUTPUT_DIR, 'label_encoder.pkl')}\")\n",
    "    print(f\"3. Training Features: {os.path.join(OUTPUT_DIR, 'train_features.npz')}\")\n",
    "    print(f\"4. Validation Features: {os.path.join(OUTPUT_DIR, 'valid_features.npz')}\")\n",
    "    print(f\"5. Training Metadata: {os.path.join(OUTPUT_DIR, 'tfidf_training_metadata.pkl')}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

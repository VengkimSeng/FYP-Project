{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 text files to process\n",
      "\n",
      "--- Stopword Loading Debug Info ---\n",
      "Total lines in file: 1040\n",
      "Empty lines skipped: 0\n",
      "Words after normalization: 1040\n",
      "Unique words after removing duplicates: 984\n",
      "Duplicates removed: 56\n",
      "\n",
      "Words that normalized to the same form:\n",
      "  'កាន់' from: ['កាន់', 'កាន់']\n",
      "  'ច្បាស់ណាស់' from: ['ច្បាស់ណាស់', 'ច្បាស់ណាស់']\n",
      "  'ទទួលយក' from: ['ទទួលយក', 'ទទួលយក']\n",
      "  'ទាំងនេះ' from: ['ទាំងនេះ', 'ទាំងនេះ']\n",
      "  'ទាំងពីរ' from: ['ទាំងពីរ', 'ទាំងពីរ']\n",
      "  ...and 50 more.\n",
      "\n",
      "Successfully loaded 984 stopwords\n",
      "Expanded to 984 stopword variations\n",
      "Loaded and expanded 984 stopword variations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Khmer text files: 100%|██████████| 15000/15000 [08:49<00:00, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopword removal statistics saved to: /Users/socheata/Documents/FYP-Khmer-Classification/Preprocess_articles/stopword_removal_stats.txt\n",
      "Debug information saved to: /Users/socheata/Documents/FYP-Khmer-Classification/Preprocess_articles/stopword_debug.txt\n",
      "Successfully processed 15000 out of 15000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Khmer Text Preprocessing with Improved Stopword Removal\n",
    "import pandas as pd\n",
    "import khmernltk\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Define paths\n",
    "# Separate base directory and data directory\n",
    "# Dynamically set BASE_DIR to the parent directory of the current notebook's directory\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"FYP-Data-Preprocessing\")\n",
    "\n",
    "# Set the processed texts output directory to the specified absolute path\n",
    "ORIGINAL_TEXTS_DIR = os.path.join(BASE_DIR, \"original_articles/texts\")\n",
    "PROCESSED_TEXTS_DIR = os.path.join(BASE_DIR, \"Preprocess_articles\")\n",
    "METADATA_PATH = os.path.join(BASE_DIR, \"original_articles/metadata.csv\")\n",
    "STOPWORDS_PATH = os.path.join(DATA_DIR, \"Khmer-Stop-Word-1000.txt\")\n",
    "STATS_OUTPUT_PATH = os.path.join(PROCESSED_TEXTS_DIR, \"stopword_removal_stats.txt\")\n",
    "DEBUG_OUTPUT_PATH = os.path.join(PROCESSED_TEXTS_DIR, \"stopword_debug.txt\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_TEXTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load metadata to get category information\n",
    "if os.path.exists(METADATA_PATH):\n",
    "    metadata_df = pd.read_csv(METADATA_PATH)\n",
    "    # Create a dictionary for quick lookup: docId -> category\n",
    "    doc_categories = dict(zip(metadata_df['docId'], metadata_df['category']))\n",
    "else:\n",
    "    print(f\"Warning: Metadata file not found at {METADATA_PATH}. Categories will be set to 'unknown'.\")\n",
    "    metadata_df = None\n",
    "    doc_categories = {}\n",
    "\n",
    "# Define Khmer character sets\n",
    "KHCONST = set(u'កខគឃងចឆជឈញដឋឌឍណតថទធនបផពភមយរលវឝឞសហឡអឣឤឥឦឧឨឩឪឫឬឭឮឯឰឱឲឳ')\n",
    "KHVOWEL = set(u'឴឵ាិីឹឺុូួើឿៀេែៃោៅ\\u17c6\\u17c7\\u17c8')\n",
    "KHSUB = set(u'្')\n",
    "KHDIAC = set(u\"\\u17c9\\u17ca\\u17cb\\u17cc\\u17cd\\u17ce\\u17cf\\u17d0\")\n",
    "KHSYM = set('៕។៛ៗ៚៙៘៖«»')\n",
    "KHNUMBER = set(u'០១២៣៤៥៦៧៨៩')\n",
    "ARABIC_NUMBER = set('0123456789')\n",
    "LATIN_CHARS = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "PUNCTUATION = set('\"!@#$%^&*()-_+=[]{};\\'\"\\\\|,.<>?/`~፡.,፣;፤፥፦፧፪፠፨')\n",
    "\n",
    "def normalize_khmer_text(text):\n",
    "    \"\"\"Normalize Khmer text for consistent processing\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Unicode normalization\n",
    "    normalized = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Remove invisible characters and control characters\n",
    "    normalized = ''.join(char for char in normalized \n",
    "                        if not unicodedata.category(char).startswith('C'))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def clean_khmer_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # First normalize the text\n",
    "    text = normalize_khmer_text(text)\n",
    "      \n",
    "    # Characters to remove: symbols, numbers, Latin chars, punctuation\n",
    "    chars_to_remove = KHSYM | KHNUMBER | ARABIC_NUMBER | LATIN_CHARS | PUNCTUATION\n",
    "    \n",
    "    # Create translation table (faster than regex for character removal)\n",
    "    translation_table = str.maketrans('', '', ''.join(chars_to_remove))\n",
    "    \n",
    "    # Apply translation to remove unwanted characters\n",
    "    text = text.translate(translation_table)\n",
    "    \n",
    "    # Normalize spaces (remove excessive spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize a single word for consistent comparison\"\"\"\n",
    "    if not word:\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode\n",
    "    word = unicodedata.normalize('NFC', word)\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    word = word.strip()\n",
    "    \n",
    "    # Remove any remaining invisible characters\n",
    "    word = ''.join(char for char in word \n",
    "                   if not unicodedata.category(char).startswith('C'))\n",
    "    \n",
    "    return word\n",
    "\n",
    "def segment_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Apply word segmentation to Khmer text using Khmer-NLTK\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append title to content before segmentation\n",
    "        # Apply word segmentation using khmernltk\n",
    "        segmented_text = khmernltk.word_tokenize(text)\n",
    "        \n",
    "        # Normalize each token\n",
    "        normalized_tokens = [normalize_word(token) for token in segmented_text]\n",
    "        \n",
    "        # Filter out empty tokens\n",
    "        normalized_tokens = [token for token in normalized_tokens if token]\n",
    "        \n",
    "        # Join with spaces to create a segmented string\n",
    "        return ' '.join(normalized_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in segmentation: {e}\")\n",
    "        # If segmentation fails, return the original text\n",
    "        return text\n",
    "\n",
    "def load_khmer_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    Load Khmer stopwords from a text file with normalization and debugging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize an empty list for stopwords\n",
    "        stopwords_list = []\n",
    "        \n",
    "        # Debug counters\n",
    "        total_lines = 0\n",
    "        empty_lines = 0\n",
    "        normalized_words = {}  # Track normalized words and their original form\n",
    "        \n",
    "        # Open and read the text file with UTF-8 encoding for Khmer characters\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Read each line and add to stopwords list\n",
    "            for line in file:\n",
    "                total_lines += 1\n",
    "                word = line.strip()\n",
    "                if not word:  # Check if the word is empty\n",
    "                    empty_lines += 1\n",
    "                    continue\n",
    "                \n",
    "                # Normalize the stopword\n",
    "                normalized_word = normalize_word(word)\n",
    "                if normalized_word:\n",
    "                    # Track if this normalized form already exists\n",
    "                    if normalized_word in normalized_words:\n",
    "                        # This is a duplicate after normalization\n",
    "                        normalized_words[normalized_word].append(word)\n",
    "                    else:\n",
    "                        normalized_words[normalized_word] = [word]\n",
    "                    stopwords_list.append(normalized_word)\n",
    "        \n",
    "        # Create set for faster lookup (removes duplicates)\n",
    "        original_count = len(stopwords_list)\n",
    "        stopwords_set = set(stopwords_list)\n",
    "        duplicates_removed = original_count - len(stopwords_set)\n",
    "        \n",
    "        # Print debug information\n",
    "        print(f\"\\n--- Stopword Loading Debug Info ---\")\n",
    "        print(f\"Total lines in file: {total_lines}\")\n",
    "        print(f\"Empty lines skipped: {empty_lines}\")\n",
    "        print(f\"Words after normalization: {original_count}\")\n",
    "        print(f\"Unique words after removing duplicates: {len(stopwords_set)}\")\n",
    "        print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "        \n",
    "        # Print details about words that normalized to the same form\n",
    "        duplicate_normalizations = {word: instances for word, instances in normalized_words.items() if len(instances) > 1}\n",
    "        if duplicate_normalizations:\n",
    "            print(\"\\nWords that normalized to the same form:\")\n",
    "            for norm_word, originals in list(duplicate_normalizations.items())[:5]:  # Show first 5 examples\n",
    "                print(f\"  '{norm_word}' from: {originals}\")\n",
    "            if len(duplicate_normalizations) > 5:\n",
    "                print(f\"  ...and {len(duplicate_normalizations) - 5} more.\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully loaded {len(stopwords_set)} stopwords\")\n",
    "        return stopwords_set\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stopwords: {e}\")\n",
    "        # Return an empty set if loading fails\n",
    "        return set()\n",
    "def create_stopword_variations(stopword):\n",
    "    \"\"\"Create variations of a stopword for better matching\"\"\"\n",
    "    variations = set()\n",
    "    \n",
    "    # Add the original\n",
    "    variations.add(stopword)\n",
    "    \n",
    "    # Add different normalizations\n",
    "    variations.add(unicodedata.normalize('NFD', stopword))\n",
    "    variations.add(unicodedata.normalize('NFKC', stopword))\n",
    "    variations.add(unicodedata.normalize('NFKD', stopword))\n",
    "    \n",
    "    # Remove variations that are empty or too short\n",
    "    variations = {var for var in variations if var and len(var) > 0}\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def load_khmer_stopwords_with_variations(file_path):\n",
    "    \"\"\"Load stopwords and create variations for better matching\"\"\"\n",
    "    base_stopwords = load_khmer_stopwords(file_path)\n",
    "    \n",
    "    # Create expanded set with variations\n",
    "    expanded_stopwords = set()\n",
    "    \n",
    "    for stopword in base_stopwords:\n",
    "        variations = create_stopword_variations(stopword)\n",
    "        expanded_stopwords.update(variations)\n",
    "    \n",
    "    print(f\"Expanded to {len(expanded_stopwords)} stopword variations\")\n",
    "    return expanded_stopwords\n",
    "\n",
    "def remove_stopwords(text, stopwords_set, debug_info=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from segmented Khmer text with improved matching\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out stopwords and count the removed ones\n",
    "    removed_count = 0\n",
    "    filtered_words = []\n",
    "    missed_stopwords = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Normalize the word for comparison\n",
    "        normalized_word = normalize_word(word)\n",
    "        \n",
    "        if normalized_word in stopwords_set:\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            # Check if word contains any stopwords as substrings\n",
    "            found_as_substring = False\n",
    "            for stopword in stopwords_set:\n",
    "                if stopword in normalized_word or normalized_word in stopword:\n",
    "                    removed_count += 1\n",
    "                    found_as_substring = True\n",
    "                    break\n",
    "            \n",
    "            if not found_as_substring:\n",
    "                filtered_words.append(word)\n",
    "                \n",
    "                # Debug: Check if this might be a missed stopword\n",
    "                if debug_info is not None:\n",
    "                    # Check for potential stopwords based on character composition\n",
    "                    if all(char in KHCONST or char in KHVOWEL or char in KHSUB for char in normalized_word):\n",
    "                        missed_stopwords.append(word)\n",
    "    \n",
    "    if debug_info is not None:\n",
    "        debug_info['missed_stopwords'] = missed_stopwords\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words), removed_count\n",
    "\n",
    "def preprocess_text(text, stopwords_set, debug_info=None):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a text\n",
    "    \"\"\"\n",
    "    cleaned = clean_khmer_text(text)\n",
    "    segmented = segment_khmer_text(cleaned)\n",
    "    filtered, removed_count = remove_stopwords(segmented, stopwords_set, debug_info)\n",
    "    return filtered, removed_count\n",
    "\n",
    "# Main function to process all text files\n",
    "def process_khmer_text_files():\n",
    "    \"\"\"\n",
    "    Process all Khmer text files with improved stopword removal\n",
    "    \"\"\"\n",
    "    # Get list of all text files\n",
    "    text_files = [f for f in os.listdir(ORIGINAL_TEXTS_DIR) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(text_files)} text files to process\")\n",
    "    \n",
    "    # Try to load stopwords with variations\n",
    "    try:\n",
    "        # Use the correct stopwords path (text file)\n",
    "        khmer_stopwords = load_khmer_stopwords_with_variations(STOPWORDS_PATH)\n",
    "        print(f\"Loaded and expanded {len(khmer_stopwords)} stopword variations\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load stopwords: {e}\")\n",
    "        khmer_stopwords = set()\n",
    "    \n",
    "    # Process each file\n",
    "    processed_count = 0\n",
    "    processing_details = []\n",
    "    debug_details = []\n",
    "    total_stopwords_removed = 0\n",
    "    \n",
    "    for filename in tqdm(text_files, desc=\"Processing Khmer text files\"):\n",
    "        try:\n",
    "            # Get docId from filename\n",
    "            doc_id = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Read the original file\n",
    "            input_path = os.path.join(ORIGINAL_TEXTS_DIR, filename)\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Combine title and content\n",
    "            parts = text.split('\\n\\n', 1)\n",
    "            title = parts[0] if parts else \"\"\n",
    "            content = parts[1] if len(parts) > 1 else \"\"\n",
    "            \n",
    "            # Combine title and content as you mentioned\n",
    "            combined_text = f\"{title} {content}\"\n",
    "            \n",
    "            # Process the combined text with debug info\n",
    "            debug_info = {'missed_stopwords': []}\n",
    "            processed_text, stopwords_removed = preprocess_text(combined_text, khmer_stopwords, debug_info)\n",
    "            \n",
    "            total_stopwords_removed += stopwords_removed\n",
    "            \n",
    "            # Store statistics\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'doc_id': doc_id,\n",
    "                'category': doc_categories.get(doc_id, 'unknown'),\n",
    "                'stopwords_removed': stopwords_removed\n",
    "            })\n",
    "            \n",
    "            # Store debug information\n",
    "            if debug_info['missed_stopwords']:\n",
    "                debug_details.append({\n",
    "                    'filename': filename,\n",
    "                    'missed_stopwords': debug_info['missed_stopwords'][:10]  # Limit to first 10\n",
    "                })\n",
    "            \n",
    "            # Save the processed text\n",
    "            output_path = os.path.join(PROCESSED_TEXTS_DIR, filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(processed_text)\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    # Save statistics\n",
    "    save_statistics(processed_count, total_stopwords_removed, processing_details, khmer_stopwords)\n",
    "    \n",
    "    # Save debug information\n",
    "    save_debug_information(debug_details, text_files[0] if text_files else None)\n",
    "    \n",
    "    print(f\"Successfully processed {processed_count} out of {len(text_files)} files\")\n",
    "\n",
    "def save_statistics(processed_count, total_stopwords_removed, processing_details, khmer_stopwords):\n",
    "    \"\"\"Save stopword removal statistics to a text file\"\"\"\n",
    "    with open(STATS_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"================================\\n\")\n",
    "        f.write(\"Khmer Text Preprocessing Statistics\\n\")\n",
    "        f.write(\"================================\\n\\n\")\n",
    "        f.write(f\"Total files processed: {processed_count}\\n\")\n",
    "        f.write(f\"Total stopwords loaded (with variations): {len(khmer_stopwords)}\\n\")\n",
    "        f.write(f\"Total stopwords removed across all files: {total_stopwords_removed}\\n\")\n",
    "        f.write(f\"Average stopwords removed per file: {total_stopwords_removed/processed_count if processed_count > 0 else 0:.2f}\\n\")\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        f.write(\"Per-file Details:\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        for detail in processing_details:\n",
    "            f.write(f\"File: {detail['filename']} (ID: {detail['doc_id']}, Category: {detail['category']})\\n\")\n",
    "            f.write(f\"  Stopwords removed: {detail['stopwords_removed']}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nStopword removal statistics saved to: {STATS_OUTPUT_PATH}\")\n",
    "\n",
    "def save_debug_information(debug_details, sample_file):\n",
    "    \"\"\"Save debug information about potentially missed stopwords\"\"\"\n",
    "    with open(DEBUG_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=====================================\\n\")\n",
    "        f.write(\"Debug: Potentially Missed Stopwords\\n\")\n",
    "        f.write(\"=====================================\\n\\n\")\n",
    "        \n",
    "        for detail in debug_details[:10]:  # Limit to first 10 files for clarity\n",
    "            f.write(f\"File: {detail['filename']}\\n\")\n",
    "            f.write(\"Potentially missed stopwords:\\n\")\n",
    "            for i, word in enumerate(detail['missed_stopwords'][:10], 1):\n",
    "                f.write(f\"  {i}. '{word}'\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Show sample processing\n",
    "        if sample_file:\n",
    "            f.write(\"\\n\\nSAMPLE PROCESSING EXAMPLE\\n\")\n",
    "            f.write(\"========================\\n\")\n",
    "            \n",
    "            try:\n",
    "                with open(os.path.join(ORIGINAL_TEXTS_DIR, sample_file), 'r', encoding='utf-8') as original:\n",
    "                    original_text = original.read()\n",
    "                \n",
    "                with open(os.path.join(PROCESSED_TEXTS_DIR, sample_file), 'r', encoding='utf-8') as processed:\n",
    "                    processed_text = processed.read()\n",
    "                \n",
    "                f.write(f\"File: {sample_file}\\n\")\n",
    "                f.write(\"\\nOriginal words (first 50):\\n\")\n",
    "                original_words = original_text.split()[:50]\n",
    "                f.write(' '.join(original_words) + \"\\n\")\n",
    "                \n",
    "                f.write(\"\\nProcessed words (first 50):\\n\")\n",
    "                processed_words = processed_text.split()[:50]\n",
    "                f.write(' '.join(processed_words) + \"\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                f.write(f\"Error showing sample: {e}\\n\")\n",
    "    \n",
    "    print(f\"Debug information saved to: {DEBUG_OUTPUT_PATH}\")\n",
    "\n",
    "# Run the processing function\n",
    "if __name__ == \"__main__\":\n",
    "    process_khmer_text_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

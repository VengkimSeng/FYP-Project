{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khmer Text Preprocessing\n",
    "\n",
    "This notebook preprocesses Khmer language text from article datasets by:\n",
    "- Removing Khmer punctuations\n",
    "- Removing special characters\n",
    "- Removing spaces (normalizing to single spaces)\n",
    "- Removing numbers (both Khmer and Arabic)\n",
    "\n",
    "The cleaned text will be saved back to the original JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import khmernltk\n",
    "from tqdm.notebook import tqdm \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required libraries\n",
    "required_libraries = ['json', 're', 'os', 'pandas', 'khmernltk', 'tqdm']\n",
    "\n",
    "# Function to install a library\n",
    "def install_library(library):\n",
    "  try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
    "  except Exception as e:\n",
    "    print(f\"Error installing {library}: {e}\")\n",
    "\n",
    "# Check and install missing libraries\n",
    "for lib in required_libraries:\n",
    "  try:\n",
    "    __import__(lib)\n",
    "  except ImportError:\n",
    "    print(f\"{lib} is not installed. Installing...\")\n",
    "    if lib == 'json' or lib == 're' or lib == 'os':\n",
    "      print(f\"{lib} is a built-in library and does not require installation.\")\n",
    "    elif lib == 'pandas':\n",
    "      install_library('pandas')\n",
    "    elif lib == 'khmernltk':\n",
    "      install_library('khmer-nltk')\n",
    "    elif lib == 'tqdm':\n",
    "      install_library('tqdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "ORIGINAL_TEXTS_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/orginal_articles/texts'\n",
    "PROCESSED_TEXTS_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/Preprocess_articles'\n",
    "METADATA_PATH = '/Users/socheata/Documents/FYP-Khmer-Classification/orginal_articles/metadata.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_TEXTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load metadata to get category information\n",
    "metadata_df = pd.read_csv(METADATA_PATH)\n",
    "# Create a dictionary for quick lookup: docId -> category\n",
    "doc_categories = dict(zip(metadata_df['docId'], metadata_df['category']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 text files to process\n",
      "Loaded 380 Khmer stopwords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1374e71b61c42649238e2f0c3b8ba92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Khmer text files:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 15000 out of 15000 files\n",
      "Processing time: 376.76 seconds\n",
      "\n",
      "Sample comparison for file: health600.txt\n",
      "\n",
      "Original title:\n",
      "អភិបាលខេត្តរតនគិរីបញ្ជាឱ្យបិទទីតាំងផលិតស្រាសទូទាំងខេត្ត ក្រោយពុលស្លាប់មនុស្ស ៣នាក់\n",
      "\n",
      "Processed title:\n",
      "អភិបាល ខេត្ត រតនគិរី បញ្ជា បិទ ទីតាំង ផលិត ស្រាស ខេត្ត ពុល ស្លាប់ មនុស្ស នាក់\n",
      "\n",
      "Original content (first 100 chars):\n",
      "អភិបាលខេត្តរតនគិរីបញ្ជាឱ្យបិទទីតាំងផលិតស្រាស នៅទូទាំងខេត្តភ្លាមៗ ក្រោយមានពលរដ្ឋចំនួន ៣នាក់បានស្លាប់ដ...\n",
      "\n",
      "Processed content (first 100 chars):\n",
      "អភិបាល ខេត្ត រតនគិរី បញ្ជា បិទ ទីតាំង ផលិត ស្រាស នៅ ខេត្ត ភ្លាម ពលរដ្ឋ នាក់ បាន ស្លាប់ សង្ស័យ ពុល ស្...\n",
      "\n",
      "Title word count: 3 → 13\n",
      "Content word count: 60 → 172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15000, 15000, 376.75803685188293)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define functions for the three main preprocessing steps\n",
    "\n",
    "def clean_khmer_text(text):\n",
    "  \"\"\"\n",
    "  Clean Khmer text by removing unwanted characters and normalizing spaces.\n",
    "  \"\"\"\n",
    "  # First, normalize all whitespace (spaces, tabs, newlines) to single spaces\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  # Define pattern for unwanted characters\n",
    "  # Include all punctuation marks, symbols, numbers and English characters\n",
    "  pattern = r\"[។៘ៗ៕៚៙៖«»៛!@#$%^&*()_\\-+=\\[\\]{};:'\\\"\\\\\\|<>?/`~០-៩0-9a-zA-Z,.]\"\n",
    "  \n",
    "  # Remove unwanted characters\n",
    "  text = re.sub(pattern, \"\", text)\n",
    "  \n",
    "  # Normalize spaces again after character removal\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  return text.strip()\n",
    "\n",
    "def remove_stopwords_from_unsegmented(text, stopwords_set):\n",
    "  \"\"\"\n",
    "  Improved stopword removal that handles Khmer text better\n",
    "  \"\"\"\n",
    "  if not stopwords_set:\n",
    "    return text\n",
    "  \n",
    "  # First normalize the text to ensure consistent spacing\n",
    "  text = ' ' + text + ' '  # Add spaces at beginning and end for boundary matching\n",
    "  \n",
    "  # Sort stopwords by length (longest first) to prevent partial matches\n",
    "  sorted_stopwords = sorted(stopwords_set, key=len, reverse=True)\n",
    "  \n",
    "  # Replace each stopword with a space\n",
    "  for word in sorted_stopwords:\n",
    "    # Need to add space boundaries for Khmer since \\b doesn't work well\n",
    "    word_with_boundaries = ' ' + word + ' '\n",
    "    text = text.replace(word_with_boundaries, ' ')\n",
    "  \n",
    "  # Clean up any resulting multiple spaces\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  return text.strip()\n",
    "\n",
    "def segment_khmer_text(text):\n",
    "  \"\"\"\n",
    "  Apply word segmentation to Khmer text using Khmer-NLTK\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Apply word segmentation using khmernltk\n",
    "    segmented_text = khmernltk.word_tokenize(text)\n",
    "    \n",
    "    # Join with spaces to create a segmented string\n",
    "    return ' '.join(segmented_text)\n",
    "  except Exception as e:\n",
    "    print(f\"Error in segmentation: {e}\")\n",
    "    # If segmentation fails, return the original text\n",
    "    return text\n",
    "\n",
    "def load_khmer_stopwords(file_path):\n",
    "  \"\"\"\n",
    "  Load Khmer stopwords from either a text file or an Excel file.\n",
    "  \n",
    "  Args:\n",
    "      file_path (str): Path to the file containing Khmer stopwords\n",
    "      \n",
    "  Returns:\n",
    "      set: A set of Khmer stopwords\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Check the file extension\n",
    "    if file_path.endswith('.txt'):\n",
    "      # For text files\n",
    "      with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "      \n",
    "      # Filter out comment lines and empty lines, and strip whitespace\n",
    "      stopwords = []\n",
    "      for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('//'):\n",
    "          stopwords.append(line)\n",
    "      \n",
    "      return set(stopwords)\n",
    "    \n",
    "    elif file_path.endswith('.xlsx'):\n",
    "      # For Excel files\n",
    "      # Read stopwords from Excel file\n",
    "      stopwords_df = pd.read_excel(file_path)\n",
    "      \n",
    "      # Extract stopwords from the DataFrame\n",
    "      stopwords_list = stopwords_df.iloc[:, 0].tolist()\n",
    "      \n",
    "      # Clean the stopwords list (remove NaN values and convert to string)\n",
    "      stopwords_list = [str(word).strip() for word in stopwords_list if isinstance(word, str)]\n",
    "      \n",
    "      return set(stopwords_list)\n",
    "    \n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported file format for {file_path}. Use .txt or .xlsx.\")\n",
    "  \n",
    "  except Exception as e:\n",
    "    print(f\"Error loading stopwords: {e}\")\n",
    "    # Return an empty set if loading fails\n",
    "    return set()\n",
    "\n",
    "def normalize_file_encoding(input_path, encoding='utf-8'):\n",
    "  \"\"\"\n",
    "  Read file with proper encoding and normalize line endings\n",
    "  \"\"\"\n",
    "  try:\n",
    "    with open(input_path, 'r', encoding=encoding) as f:\n",
    "      text = f.read()\n",
    "    \n",
    "    # Normalize line endings to \\n\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    \n",
    "    return text\n",
    "  except UnicodeDecodeError:\n",
    "    # Try with different encoding if utf-8 fails\n",
    "    try:\n",
    "      with open(input_path, 'r', encoding='utf-16') as f:\n",
    "        text = f.read()\n",
    "      return text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    except:\n",
    "      # If all else fails, read as binary and decode with errors ignored\n",
    "      with open(input_path, 'rb') as f:\n",
    "        text = f.read().decode('utf-8', errors='ignore')\n",
    "      return text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "def preprocess_text(text, stopwords_set):\n",
    "  \"\"\"\n",
    "  Apply all preprocessing steps to a text in the order:\n",
    "  1. Segment words (to identify word boundaries first)\n",
    "  2. Clean text (remove punctuation while preserving word structure)\n",
    "  3. Remove stopwords\n",
    "  \"\"\"\n",
    "  segmented = segment_khmer_text(text)\n",
    "  cleaned = clean_khmer_text(segmented)\n",
    "  filtered = remove_stopwords_from_unsegmented(cleaned, stopwords_set)\n",
    "  return filtered\n",
    "\n",
    "\n",
    "# Function to process a specific number of files\n",
    "def process_khmer_text_files(max_files=None, output_dir=None):\n",
    "  \"\"\"\n",
    "  Process Khmer text files with preprocessing steps.\n",
    "  \n",
    "  Args:\n",
    "      max_files (int, optional): Maximum number of files to process. If None, process all files.\n",
    "      output_dir (str, optional): Directory to save processed files. If None, use default.\n",
    "  \n",
    "  Returns:\n",
    "      tuple: (processed_count, total_files, processing_time)\n",
    "  \"\"\"\n",
    "  import time\n",
    "  start_time = time.time()\n",
    "  \n",
    "  # Use specified output directory or default\n",
    "  output_directory = output_dir if output_dir is not None else PROCESSED_TEXTS_DIR\n",
    "  os.makedirs(output_directory, exist_ok=True)\n",
    "  \n",
    "  # Get list of all text files\n",
    "  text_files = [f for f in os.listdir(ORIGINAL_TEXTS_DIR) if f.endswith('.txt')]\n",
    "  \n",
    "  # Limit the number of files if max_files is specified\n",
    "  if max_files is not None and max_files > 0:\n",
    "    text_files = text_files[:min(max_files, len(text_files))]\n",
    "  \n",
    "  print(f\"Found {len(text_files)} text files to process\")\n",
    "  \n",
    "  # Try to load stopwords from text file instead of Excel\n",
    "  try:\n",
    "    stopwords_path = \"khmer_stopwords.txt\"  # Use text file instead of Excel\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "      khmer_stopwords = set(f.read().splitlines())\n",
    "    print(f\"Loaded {len(khmer_stopwords)} Khmer stopwords\")\n",
    "  except Exception as e:\n",
    "    print(f\"Warning: Could not load stopwords: {e}\")\n",
    "    # Try loading from Excel as fallback\n",
    "    try:\n",
    "      stopwords_path = \"khmer stopwords-corpus-385.xlsx\"\n",
    "      khmer_stopwords = load_khmer_stopwords(stopwords_path)\n",
    "      print(f\"Loaded {len(khmer_stopwords)} Khmer stopwords from Excel\")\n",
    "    except Exception:\n",
    "      khmer_stopwords = set()\n",
    "  \n",
    "  # Process each file\n",
    "  processed_count = 0\n",
    "  \n",
    "  # Update this part in your process_khmer_text_files function\n",
    "  for filename in tqdm(text_files, desc=\"Processing Khmer text files\"):\n",
    "    try:\n",
    "      # Get docId from filename\n",
    "      doc_id = os.path.splitext(filename)[0]\n",
    "      \n",
    "      # Read the original file with encoding normalization\n",
    "      input_path = os.path.join(ORIGINAL_TEXTS_DIR, filename)\n",
    "      text = normalize_file_encoding(input_path)\n",
    "      \n",
    "      # Split into title and content\n",
    "      parts = text.split('\\n\\n', 1)\n",
    "      title = parts[0] if parts else \"\"\n",
    "      content = parts[1] if len(parts) > 1 else \"\"\n",
    "      \n",
    "      # Process both title and content\n",
    "      processed_title = preprocess_text(title, khmer_stopwords)\n",
    "      processed_content = preprocess_text(content, khmer_stopwords)\n",
    "      \n",
    "      # Save the processed text\n",
    "      output_path = os.path.join(output_directory, filename)\n",
    "      with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # Save the processed title and content\n",
    "        f.write(f\"{processed_title}\\n\\n{processed_content}\")\n",
    "      \n",
    "      processed_count += 1\n",
    "      \n",
    "    except Exception as e:\n",
    "      print(f\"Error processing {filename}: {str(e)}\")\n",
    "  \n",
    "  # Calculate processing time\n",
    "  processing_time = time.time() - start_time\n",
    "  \n",
    "  print(f\"Successfully processed {processed_count} out of {len(text_files)} files\")\n",
    "  print(f\"Processing time: {processing_time:.2f} seconds\")\n",
    "  \n",
    "  # Show a sample of a processed file\n",
    "  if processed_count > 0:\n",
    "    # Display sample comparison (same as before)\n",
    "    sample_file = text_files[0]\n",
    "    print(f\"\\nSample comparison for file: {sample_file}\")\n",
    "    \n",
    "    # Read original\n",
    "    with open(os.path.join(ORIGINAL_TEXTS_DIR, sample_file), 'r', encoding='utf-8') as f:\n",
    "      original_text = f.read()\n",
    "    \n",
    "    # Read processed\n",
    "    with open(os.path.join(output_directory, sample_file), 'r', encoding='utf-8') as f:\n",
    "      processed_text = f.read()\n",
    "    \n",
    "    # Split to get title and content\n",
    "    original_parts = original_text.split('\\n\\n', 1)\n",
    "    processed_parts = processed_text.split('\\n\\n', 1)\n",
    "    \n",
    "    original_title = original_parts[0] if original_parts else \"\"\n",
    "    original_content = original_parts[1] if len(original_parts) > 1 else \"\"\n",
    "    \n",
    "    processed_title = processed_parts[0] if processed_parts else \"\"\n",
    "    processed_content = processed_parts[1] if len(processed_parts) > 1 else \"\"\n",
    "    \n",
    "    print(\"\\nOriginal title:\")\n",
    "    print(original_title)\n",
    "    \n",
    "    print(\"\\nProcessed title:\")\n",
    "    print(processed_title)\n",
    "    \n",
    "    print(\"\\nOriginal content (first 100 chars):\")\n",
    "    print(original_content[:100] + \"...\")\n",
    "    \n",
    "    print(\"\\nProcessed content (first 100 chars):\")\n",
    "    print(processed_content[:100] + \"...\")\n",
    "    \n",
    "    # Print statistics\n",
    "    original_title_word_count = len(original_title.split())\n",
    "    processed_title_word_count = len(processed_title.split())\n",
    "    \n",
    "    original_content_word_count = len(original_content.split())\n",
    "    processed_content_word_count = len(processed_content.split())\n",
    "    \n",
    "    print(f\"\\nTitle word count: {original_title_word_count} → {processed_title_word_count}\")\n",
    "    print(f\"Content word count: {original_content_word_count} → {processed_content_word_count}\")\n",
    "  \n",
    "  return processed_count, len(text_files), processing_time\n",
    "\n",
    "# Function to run tests on different batch sizes\n",
    "def run_batch_tests():\n",
    "  \"\"\"\n",
    "  Run preprocessing on different batch sizes (10, 100, 1000, 5000, 10000, 15000)\n",
    "  and compare performance\n",
    "  \"\"\"\n",
    "  batch_sizes = [10, 100, 1000, 5000, 10000, 15000]\n",
    "  results = []\n",
    "  \n",
    "  for batch_size in batch_sizes:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing with batch size: {batch_size}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Create a new output directory for this batch\n",
    "    batch_output_dir = f\"{PROCESSED_TEXTS_DIR}_{batch_size}\"\n",
    "    \n",
    "    # Process the batch\n",
    "    processed, total, proc_time = process_khmer_text_files(max_files=batch_size, output_dir=batch_output_dir)\n",
    "    \n",
    "    # Store results\n",
    "    results.append((batch_size, processed, proc_time))\n",
    "  \n",
    "  # Print summary of results\n",
    "  print(\"\\n\\n\")\n",
    "  print(f\"{'='*70}\")\n",
    "  print(\"Batch Processing Results Summary\")\n",
    "  print(f\"{'='*70}\")\n",
    "  print(f\"{'Batch Size':<15}{'Files Processed':<20}{'Processing Time':<20}{'Time per File':<15}\")\n",
    "  print(f\"{'-'*70}\")\n",
    "  \n",
    "  for batch_size, processed, proc_time in results:\n",
    "    time_per_file = proc_time / processed if processed > 0 else 0\n",
    "    print(f\"{batch_size:<15}{processed:<20}{proc_time:.2f}s{'':<10}{time_per_file:.4f}s{'':<5}\")\n",
    "\n",
    "# Run a specific batch size test\n",
    "def run_specific_test(size):\n",
    "  \"\"\"\n",
    "  Process a specific number of files\n",
    "  \n",
    "  Args:\n",
    "      size (int): Number of files to process\n",
    "  \"\"\"\n",
    "  print(f\"Processing {size} files...\")\n",
    "  process_khmer_text_files(max_files=size)\n",
    "\n",
    "\n",
    "  \n",
    "# Choose how to run the tests\n",
    "\n",
    "# Option 1: Run all batch sizes (10, 100, 1000, 5000, 10000, 15000)\n",
    "# WARNING: This may take a long time to complete\n",
    "# run_batch_tests()\n",
    "\n",
    "# Option 2: Run a specific batch size (uncomment and modify the line below)\n",
    "# run_specific_test(100)  # Process 100 articles\n",
    "\n",
    "# Option 3: Run the default processing (all files)\n",
    "process_khmer_text_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e8c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText sentence embeddings and labels...\n",
      "Loaded 15000 samples with 100-dim FastText features (PCA reduced).\n",
      "Categories: ['economic', 'environment', 'health', 'politic', 'sport', 'technology']\n",
      "Training set: 12000 samples\n",
      "Test set: 3000 samples\n",
      "\n",
      "================================================================================\n",
      "Training models on TF-IDF weighted FastText features (100D)\n",
      "================================================================================\n",
      "\n",
      "Optimizing SVM hyperparameters...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 267\u001b[0m\n\u001b[1;32m    252\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    256\u001b[0m }\n\u001b[1;32m    258\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    259\u001b[0m     SVC(probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m    260\u001b[0m     param_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    265\u001b[0m )\n\u001b[0;32m--> 267\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest cross-validation score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "FASTTEXT_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/FastText_Features_Fun'\n",
    "MODELS_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/Models/fasttext_models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Load FastText features and labels\n",
    "print(\"Loading FastText sentence embeddings and labels...\")\n",
    "X = np.load(os.path.join(FASTTEXT_DIR, 'sentence_embeddings.npy'))  # Already 100D from PCA\n",
    "y = np.load(os.path.join(FASTTEXT_DIR, 'labels.npy'), allow_pickle=True)\n",
    "doc_ids = pd.read_csv(os.path.join(FASTTEXT_DIR, 'labels.csv'))['docId'].values\n",
    "\n",
    "# Load the preprocessing artifacts (for production prediction pipeline)\n",
    "scaler = joblib.load(os.path.join(FASTTEXT_DIR, 'embedding_scaler.joblib'))\n",
    "pca_reducer = joblib.load(os.path.join(FASTTEXT_DIR, 'pca_reducer.joblib'))\n",
    "word_weights = joblib.load(os.path.join(FASTTEXT_DIR, 'word_weights.joblib'))\n",
    "\n",
    "# Get unique categories from labels\n",
    "categories = sorted(np.unique(y))\n",
    "print(f\"Loaded {X.shape[0]} samples with {X.shape[1]}-dim FastText features (PCA reduced).\")\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, doc_ids_train, doc_ids_test = train_test_split(\n",
    "    X, y, doc_ids, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Shift FastText embeddings to be non-negative for MNB (not recommended for production)\n",
    "X_train_shift = X_train - X_train.min()\n",
    "X_test_shift = X_test - X_train.min()\n",
    "\n",
    "# Function to generate learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=5, n_jobs=None, \n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                      train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Function to train and evaluate a model with cross-validation\n",
    "def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5-fold Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"CV accuracies: {cv_scores}\")\n",
    "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model, os.path.join(MODELS_DIR, f'{model_name.lower().replace(\" \", \"_\")}.joblib'))\n",
    "    print(f\"{model_name} model saved\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=categories)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODELS_DIR, f\"{model_name.lower().replace(' ', '_')}_confusion_matrix.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=categories))\n",
    "    \n",
    "    # Generate learning curves\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_learning_curve(model, f'Learning Curve - {model_name}', X_train, y_train, cv=5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODELS_DIR, f\"{model_name.lower().replace(' ', '_')}_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC curves (One-vs-Rest)\n",
    "    try:\n",
    "        print(\"\\nGenerating ROC curves (one-vs-rest)...\")\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(y)\n",
    "        y_test_bin = lb.transform(y_test)\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            y_score = model.decision_function(X_test)\n",
    "        else:\n",
    "            y_score = model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate ROC curve and ROC area for each class\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "        n_classes = len(categories)\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # If there are many categories, plot only the first 10 for clarity\n",
    "        if len(categories) > 10:\n",
    "            for i, category in enumerate(categories[:10]):\n",
    "                plt.plot(fpr[i], tpr[i], lw=2, label=f'{category} (AUC = {roc_auc[i]:.2f})')\n",
    "        else:\n",
    "            for i, category in enumerate(categories):\n",
    "                plt.plot(fpr[i], tpr[i], lw=2, label=f'{category} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves (One-vs-Rest) - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(MODELS_DIR, f\"{model_name.lower().replace(' ', '_')}_roc_curves.png\"))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate ROC curves. Error: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time,\n",
    "        'train_accuracy': train_accuracy\n",
    "    }\n",
    "\n",
    "# Error analysis\n",
    "def analyze_errors(model, X_test, y_test, categories, doc_ids_test, top_n=10):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Error Analysis for {model.__class__.__name__}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    misclassified_indices = np.where(np.array(y_test) != np.array(y_pred))[0]\n",
    "    if len(misclassified_indices) == 0:\n",
    "        print(\"No misclassified instances found.\")\n",
    "        return\n",
    "    print(f\"Found {len(misclassified_indices)} misclassified instances out of {len(y_test)} test samples.\")\n",
    "    \n",
    "    # Group errors by category\n",
    "    errors_by_category = {}\n",
    "    for idx in misclassified_indices:\n",
    "        true_category = y_test[idx]\n",
    "        pred_category = y_pred[idx]\n",
    "        if true_category not in errors_by_category:\n",
    "            errors_by_category[true_category] = []\n",
    "        errors_by_category[true_category].append(pred_category)\n",
    "    \n",
    "    # Calculate error rates\n",
    "    error_rates = {}\n",
    "    for category in categories:\n",
    "        category_count = sum(1 for y in y_test if y == category)\n",
    "        if category_count == 0:\n",
    "            continue\n",
    "        misclassified_count = len(errors_by_category.get(category, []))\n",
    "        error_rate = misclassified_count / category_count\n",
    "        error_rates[category] = error_rate\n",
    "    \n",
    "    # Show categories with highest error rates\n",
    "    print(\"\\nCategories with Highest Error Rates:\")\n",
    "    sorted_categories = sorted(error_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "    for category, error_rate in sorted_categories[:top_n]:\n",
    "        correct_count = sum(1 for y in y_test if y == category) - len(errors_by_category.get(category, []))\n",
    "        total_count = sum(1 for y in y_test if y == category)\n",
    "        print(f\"{category}: {error_rate:.2%} error rate ({correct_count}/{total_count} correctly classified)\")\n",
    "    \n",
    "    # Analyze confusion pairs\n",
    "    confusion_pairs = {}\n",
    "    for true_category, errors in errors_by_category.items():\n",
    "        for pred_category in errors:\n",
    "            pair = (true_category, pred_category)\n",
    "            confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    print(\"\\nMost Common Confusion Pairs (True -> Predicted):\")\n",
    "    sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    for (true_cat, pred_cat), count in sorted_pairs[:top_n]:\n",
    "        print(f\"{true_cat} -> {pred_cat}: {count} instances\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training models on TF-IDF weighted FastText features (100D)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Optimized SVM with grid search\n",
    "print(\"\\nOptimizing SVM hyperparameters...\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train optimized SVM\n",
    "svm_model = grid_search.best_estimator_\n",
    "svm_results = train_and_evaluate_model(\n",
    "    \"SVM (FastText 100D)\", svm_model, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# MultinomialNB with shifted features\n",
    "mnb_model = MultinomialNB(alpha=0.1)  # Slight smoothing adjustment\n",
    "mnb_results = train_and_evaluate_model(\n",
    "    \"MNB (FastText 100D, shifted)\", mnb_model, X_train_shift, y_train, X_test_shift, y_test\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_results = train_and_evaluate_model(\n",
    "    \"RF (FastText 100D)\", rf_model, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Ensemble Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', SVC(probability=True, **grid_search.best_params_, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_results = train_and_evaluate_model(\n",
    "    \"Ensemble (SVM+RF)\", voting_clf, X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "results = [svm_results, mnb_results, rf_results, voting_results]\n",
    "model_comparison = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison (FastText Features):\")\n",
    "comparison_cols = ['model_name', 'cv_mean', 'accuracy', 'precision', 'recall', 'f1', 'training_time']\n",
    "print(model_comparison[comparison_cols])\n",
    "\n",
    "# Plot model comparison\n",
    "metrics = ['cv_mean', 'accuracy', 'precision', 'recall', 'f1']\n",
    "metric_labels = ['CV Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "model_names = [result['model_name'] for result in results]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "colors = ['royalblue', 'firebrick', 'forestgreen', 'darkorange']\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    values = [result[metric] for metric in metrics]\n",
    "    plt.bar(x + (i-1.5)*width, values, width, label=result['model_name'], color=colors[i])\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison: FastText 100D Features')\n",
    "plt.xticks(x, metric_labels)\n",
    "plt.legend(loc='lower left', bbox_to_anchor=(0, -0.15), ncol=2)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.savefig(os.path.join(MODELS_DIR, 'fasttext_model_comparison.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save best model\n",
    "all_accuracies = [r['accuracy'] for r in results]\n",
    "best_model_idx = np.argmax(all_accuracies)\n",
    "best_model_name = results[best_model_idx]['model_name']\n",
    "\n",
    "# Determine which model was best\n",
    "best_model = None\n",
    "if best_model_idx == 0:\n",
    "    best_model = svm_model\n",
    "elif best_model_idx == 1:\n",
    "    best_model = mnb_model\n",
    "elif best_model_idx == 2:\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    best_model = voting_clf\n",
    "\n",
    "# Save best model with preprocessing info\n",
    "model_info = {\n",
    "    'model': best_model,\n",
    "    'features_dim': X_train.shape[1],\n",
    "    'categories': categories,\n",
    "    'is_mnb': isinstance(best_model, MultinomialNB)\n",
    "}\n",
    "joblib.dump(model_info, os.path.join(MODELS_DIR, 'best_model.joblib'))\n",
    "print(f\"Best model ({best_model_name}) saved for production use\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Error Analysis for Best Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_model_idx == 0:\n",
    "    analyze_errors(svm_model, X_test, y_test, categories, doc_ids_test)\n",
    "elif best_model_idx == 1:\n",
    "    analyze_errors(mnb_model, X_test_shift, y_test, categories, doc_ids_test)\n",
    "elif best_model_idx == 2:\n",
    "    analyze_errors(rf_model, X_test, y_test, categories, doc_ids_test)\n",
    "else:\n",
    "    analyze_errors(voting_clf, X_test, y_test, categories, doc_ids_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training and Evaluation Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"All models saved in: {MODELS_DIR}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(\"\\nUse the following code to load and use the best model for prediction:\")\n",
    "print(\"```python\")\n",
    "print(\"import joblib\")\n",
    "print(\"import numpy as np\")\n",
    "print(\"from gensim.models.fasttext import load_facebook_model\")\n",
    "print(\"\")\n",
    "print(\"# Load model info (contains model and preprocessing details)\")\n",
    "print(\"model_info = joblib.load('/path/to/best_model.joblib')\")\n",
    "print(\"model = model_info['model']\")\n",
    "print(\"categories = model_info['categories']\")\n",
    "print(\"is_mnb = model_info['is_mnb']\")\n",
    "print(\"\")\n",
    "print(\"# Load preprocessing components\")\n",
    "print(\"ft = load_facebook_model('cc.km.300.bin')  # Original FastText model\")\n",
    "print(\"scaler = joblib.load('embedding_scaler.joblib')\")\n",
    "print(\"pca = joblib.load('pca_reducer.joblib')\")\n",
    "print(\"word_weights = joblib.load('word_weights.joblib')\")\n",
    "print(\"\")\n",
    "print(\"# Function to get weighted embeddings for new text\")\n",
    "print(\"def get_embedding(text):\")\n",
    "print(\"    words = text.strip().split()\")\n",
    "print(\"    vectors = []\")\n",
    "print(\"    weights = []\")\n",
    "print(\"    \")\n",
    "print(\"    for word in words:\")\n",
    "print(\"        if word in ft.wv:\")\n",
    "print(\"            vectors.append(ft.wv[word])\")\n",
    "print(\"            weights.append(word_weights.get(word, 1.0))\")\n",
    "print(\"    \")\n",
    "print(\"    if not vectors:\")\n",
    "print(\"        return np.zeros(ft.vector_size)\")\n",
    "print(\"    \")\n",
    "print(\"    weights = np.array(weights)\")\n",
    "print(\"    sum_weights = np.sum(weights)\")\n",
    "print(\"    \")\n",
    "print(\"    if sum_weights > 0:\")\n",
    "print(\"        weights = weights / sum_weights\")\n",
    "print(\"        emb = np.average(vectors, weights=weights, axis=0)\")\n",
    "print(\"    else:\")\n",
    "print(\"        emb = np.mean(vectors, axis=0)\")\n",
    "print(\"        \")\n",
    "print(\"    # Apply same preprocessing as training\")\n",
    "print(\"    emb_scaled = scaler.transform(emb.reshape(1, -1))\")\n",
    "print(\"    emb_reduced = pca.transform(emb_scaled)\")\n",
    "print(\"    \")\n",
    "print(\"    if is_mnb:  # For MNB models\")\n",
    "print(\"        # Use same minimum value as training\")\n",
    "print(\"        min_value = ... # Need to store this from training\")\n",
    "print(\"        emb_reduced = emb_reduced - min_value\")\n",
    "print(\"        \")\n",
    "print(\"    return emb_reduced\")\n",
    "print(\"\")\n",
    "print(\"# Example prediction\")\n",
    "print(\"text = 'ខ្ញុំចូលចិត្តអានសៀវភៅខ្មែរ'\")\n",
    "print(\"embedding = get_embedding(text)\")\n",
    "print(\"prediction = model.predict(embedding)\")\n",
    "print(\"print(f'Predicted category: {prediction[0]}')\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

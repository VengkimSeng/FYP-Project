{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e833eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (913131084.py, line 161)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 161\u001b[0;36m\u001b[0m\n\u001b[0;31m    def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix Khmer font rendering issues\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def setup_khmer_font():\n",
    "    try:\n",
    "        # Try Mac Khmer font\n",
    "        khmer_font_path = '/Library/Fonts/Khmer MN.ttc'\n",
    "        if not os.path.exists(khmer_font_path):\n",
    "            print(\"Khmer MN font not found. Installing Noto Sans Khmer...\")\n",
    "            !pip install font-noto\n",
    "            import font_noto\n",
    "            khmer_font_path = font_noto.REGULAR\n",
    "        \n",
    "        # Register font\n",
    "        khmer_font_prop = fm.FontProperties(fname=khmer_font_path)\n",
    "        plt.rcParams['font.family'] = 'sans-serif'\n",
    "        plt.rcParams['font.sans-serif'] = [khmer_font_prop.get_name()] + plt.rcParams['font.sans-serif']\n",
    "        print(f\"Khmer font configured successfully: {khmer_font_prop.get_name()}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not configure Khmer font: {e}\")\n",
    "        print(\"Visualizations may not display Khmer characters correctly\")\n",
    "        return False\n",
    "\n",
    "# Set up Khmer font\n",
    "has_khmer_font = setup_khmer_font()\n",
    "\n",
    "# Define paths\n",
    "FASTTEXT_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/FastText_Features'\n",
    "MODELS_DIR = '/Users/socheata/Documents/FYP-Khmer-Classification/Models/fasttext_models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Load FastText features and labels\n",
    "print(\"Loading FastText features and labels...\")\n",
    "X = np.load(os.path.join(FASTTEXT_DIR, 'embeddings.npy'))           # shape: (n_samples, 300)\n",
    "y = np.load(os.path.join(FASTTEXT_DIR, 'labels.npy'))               # shape: (n_samples,)\n",
    "doc_ids = np.load(os.path.join(FASTTEXT_DIR, 'doc_ids.npy'))        # shape: (n_samples,)\n",
    "label_encoder = joblib.load(os.path.join(FASTTEXT_DIR, 'label_encoder.pkl'))\n",
    "\n",
    "# Load word embeddings per document for enhanced features\n",
    "print(\"Loading word-level embeddings from pickle file...\")\n",
    "with open(os.path.join(FASTTEXT_DIR, 'word_embeddings_per_doc.pkl'), 'rb') as f:\n",
    "    word_embeddings_per_doc = pickle.load(f)\n",
    "print(f\"Loaded word embeddings for {len(word_embeddings_per_doc)} documents\")\n",
    "\n",
    "# If y is integer-encoded, decode to string labels\n",
    "if np.issubdtype(y.dtype, np.integer):\n",
    "    y = label_encoder.inverse_transform(y)\n",
    "categories = list(label_encoder.classes_)\n",
    "print(f\"Loaded {X.shape[0]} samples with {X.shape[1]}-dim FastText features.\")\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "# Function to extract additional features from word embeddings\n",
    "def extract_word_embedding_features(doc_ids, word_emb_dict):\n",
    "    \"\"\"Extract statistical features from word embeddings for each document\"\"\"\n",
    "    features = []\n",
    "    for doc_id in doc_ids:\n",
    "        doc_id_str = str(doc_id)\n",
    "        if doc_id_str in word_emb_dict and len(word_emb_dict[doc_id_str]) > 0:\n",
    "            # Get all word embeddings for this document\n",
    "            word_vectors = np.array(list(word_emb_dict[doc_id_str].values()))\n",
    "            \n",
    "            # Calculate statistical features from word embeddings\n",
    "            features_dict = {\n",
    "                'mean': np.mean(word_vectors, axis=0),\n",
    "                'variance': np.var(word_vectors, axis=0),  \n",
    "                'max': np.max(word_vectors, axis=0),\n",
    "                'min': np.min(word_vectors, axis=0),\n",
    "                'document_length': np.array([len(word_emb_dict[doc_id_str])])\n",
    "            }\n",
    "            \n",
    "            # Dimensionality reduction for variance, max, min to keep feature count manageable\n",
    "            reduced_features = np.concatenate([\n",
    "                features_dict['mean'][:50],      # First 50 dimensions of mean vector\n",
    "                features_dict['variance'][:20],  # First 20 dimensions of variance\n",
    "                features_dict['max'][:15],       # First 15 dimensions of max values\n",
    "                features_dict['min'][:15],       # First 15 dimensions of min values\n",
    "                features_dict['document_length'] # Document length (word count)\n",
    "            ])\n",
    "            features.append(reduced_features)\n",
    "        else:\n",
    "            # Use zeros for documents with no word embeddings\n",
    "            placeholder = np.zeros(101)  # 50+20+15+15+1\n",
    "            features.append(placeholder)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Extract enhanced features\n",
    "print(\"Extracting enhanced features from word embeddings...\")\n",
    "word_level_features = extract_word_embedding_features(doc_ids, word_embeddings_per_doc)\n",
    "print(f\"Generated word-level statistical features: {word_level_features.shape[1]} dimensions\")\n",
    "\n",
    "# Combine with document embeddings\n",
    "X_enhanced = np.hstack((X, word_level_features))\n",
    "print(f\"Combined feature set dimensions: {X_enhanced.shape[1]}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, doc_ids_train, doc_ids_test = train_test_split(\n",
    "    X_enhanced, y, doc_ids, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Normalize features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = Counter(y_train)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "for cls, count in class_counts.most_common():\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "# Apply SMOTE for class balancing if needed\n",
    "if len(set(class_counts.values())) > 1:\n",
    "    print(\"\\nApplying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    try:\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"After SMOTE: {X_train_balanced.shape[0]} samples\")\n",
    "        # Use balanced dataset if SMOTE succeeds\n",
    "        X_train_scaled = X_train_balanced\n",
    "        y_train = y_train_balanced\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE failed: {e}. Continuing with imbalanced data.\")\n",
    "\n",
    "# For MNB, we need non-negative features\n",
    "X_train_shifted = X_train_scaled - X_train_scaled.min()\n",
    "X_test_shifted = X_test_scaled - X_train_scaled.min()\n",
    "\n",
    "# Rest of your existing functions...\n",
    "# Function to generate learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=5, n_jobs=None, \n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    # ...existing code...\n",
    "\n",
    "# Function to train and evaluate a model with cross-validation\n",
    "def train_and_evaluate_model(model_name, model, X_train, y_train, X_test, y_test):\n",
    "    # ...existing code...\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training models with enhanced FastText features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "print(\"\\nPerforming grid search for SVM hyperparameters...\")\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_grid_svm,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_svm.fit(X_train_scaled, y_train)\n",
    "print(f\"Best SVM parameters: {grid_svm.best_params_}\")\n",
    "svm_tuned = grid_svm.best_estimator_\n",
    "\n",
    "# SVM with Enhanced Features\n",
    "svm_results = train_and_evaluate_model(\n",
    "    \"SVM (Enhanced FastText)\", svm_tuned, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "# Random Forest with Enhanced Features\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_results = train_and_evaluate_model(\n",
    "    \"RandomForest (Enhanced FastText)\", rf_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "# MultinomialNB with Enhanced Features\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_results = train_and_evaluate_model(\n",
    "    \"MNB (Enhanced FastText)\", mnb_model, X_train_shifted, y_train, X_test_shifted, y_test\n",
    ")\n",
    "\n",
    "# Create Ensemble model\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_tuned),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[2, 1],  # Give more weight to SVM if it performs better\n",
    ")\n",
    "\n",
    "ensemble_results = train_and_evaluate_model(\n",
    "    \"Ensemble (SVM+RF)\", ensemble_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "results = [svm_results, rf_results, mnb_results, ensemble_results]\n",
    "model_comparison = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison (Enhanced FastText Features):\")\n",
    "comparison_cols = ['model_name', 'cv_mean', 'accuracy', 'precision', 'recall', 'f1', 'training_time']\n",
    "print(model_comparison[comparison_cols])\n",
    "\n",
    "# Plot model comparison\n",
    "metrics = ['cv_mean', 'accuracy', 'precision', 'recall', 'f1']\n",
    "metric_labels = ['CV Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "model_names = [result['model_name'] for result in results]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2  # Smaller width since we have more models\n",
    "colors = ['royalblue', 'forestgreen', 'firebrick', 'darkorange']\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    values = [result[metric] for metric in metrics]\n",
    "    plt.bar(x + (i-1.5)*width, values, width, label=result['model_name'], color=colors[i])\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison: Enhanced FastText Features')\n",
    "plt.xticks(x, metric_labels)\n",
    "plt.legend(loc='lower left', bbox_to_anchor=(0, -0.15), ncol=2)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.savefig(os.path.join(MODELS_DIR, 'enhanced_fasttext_model_comparison.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save best model and feature transformer\n",
    "all_accuracies = [r['accuracy'] for r in results]\n",
    "best_model_idx = np.argmax(all_accuracies)\n",
    "best_model_name = results[best_model_idx]['model_name']\n",
    "\n",
    "# Determine which model is the best based on index\n",
    "if best_model_idx == 0:\n",
    "    best_model = svm_tuned\n",
    "elif best_model_idx == 1:\n",
    "    best_model = rf_model\n",
    "elif best_model_idx == 2:\n",
    "    best_model = mnb_model\n",
    "else:\n",
    "    best_model = ensemble_model\n",
    "\n",
    "# Save necessary components for production\n",
    "joblib.dump(best_model, os.path.join(MODELS_DIR, 'best_model.joblib'))\n",
    "joblib.dump(label_encoder, os.path.join(MODELS_DIR, 'label_encoder.joblib'))\n",
    "joblib.dump(scaler, os.path.join(MODELS_DIR, 'feature_scaler.joblib'))\n",
    "\n",
    "# Save function for feature extraction\n",
    "with open(os.path.join(MODELS_DIR, 'feature_extraction_func.pkl'), 'wb') as f:\n",
    "    pickle.dump(extract_word_embedding_features, f)\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved for production use\")\n",
    "print(f\"Also saved: label encoder, feature scaler, and feature extraction function\")\n",
    "\n",
    "# Error analysis\n",
    "def analyze_errors(model, X_test, y_test, categories, doc_ids_test, top_n=10):\n",
    "    # ...existing code...\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Error Analysis for Best Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the right features for the chosen model\n",
    "if isinstance(best_model, MultinomialNB):\n",
    "    analyze_errors(best_model, X_test_shifted, y_test, categories, doc_ids_test)\n",
    "else:\n",
    "    analyze_errors(best_model, X_test_scaled, y_test, categories, doc_ids_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training and Evaluation Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"All models and supporting files saved in: {MODELS_DIR}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(\"\\nUse the following code to load and use the best model for prediction:\")\n",
    "print(\"```python\")\n",
    "print(\"import joblib\")\n",
    "print(\"import pickle\")\n",
    "print(\"import numpy as np\")\n",
    "print(\"# Load components\")\n",
    "print(\"model = joblib.load('path/to/best_model.joblib')\")\n",
    "print(\"label_encoder = joblib.load('path/to/label_encoder.joblib')\")\n",
    "print(\"scaler = joblib.load('path/to/feature_scaler.joblib')\")\n",
    "print(\"with open('path/to/feature_extraction_func.pkl', 'rb') as f:\")\n",
    "print(\"    extract_features = pickle.load(f)\")\n",
    "print(\"# For prediction:\")\n",
    "print(\"# 1. Get document embeddings (X_new) and word embeddings (word_emb)\")\n",
    "print(\"# 2. Extract word-level features\")\n",
    "print(\"word_features = extract_features(doc_ids, word_emb)\")\n",
    "print(\"# 3. Combine features\")\n",
    "print(\"X_combined = np.hstack((X_new, word_features))\")\n",
    "print(\"# 4. Scale features\")\n",
    "print(\"X_scaled = scaler.transform(X_combined)\")\n",
    "print(\"# 5. For MNB, shift values to be non-negative\")\n",
    "print(\"if isinstance(model, MultinomialNB):\")\n",
    "print(\"    X_scaled = X_scaled - X_scaled.min()\")\n",
    "print(\"# 6. Predict\")\n",
    "print(\"y_pred = model.predict(X_scaled)\")\n",
    "print(\"labels = label_encoder.inverse_transform(y_pred)\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
